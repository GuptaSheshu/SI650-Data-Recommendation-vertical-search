{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# setting the path for joining multiple files\n",
        "files = os.path.join(\"/content/drive/MyDrive/SI650/650 Project/SI650-Data-Recommendation-vertical-search/\", \"amazon_raw_data*.csv\")\n",
        "\n",
        "# list of merged files returned\n",
        "files = glob.glob(files)\n",
        "\n",
        "# joining files with concat and read_csv\n",
        "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
        "df = df.dropna(subset = ['URL'])\n",
        "df.to_csv(\"amazon_general_queries_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI2IYk6_mZN5",
        "outputId": "b7975e74-3e5c-4c5b-e4bf-52a42cac462e"
      },
      "id": "oI2IYk6_mZN5",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5e9dd6e0",
      "metadata": {
        "id": "5e9dd6e0"
      },
      "outputs": [],
      "source": [
        "# output_file = open(\"amazon_data.csv\",\"a\")\n",
        "url_data = pd.read_csv(\"amazon_general_queries_data.csv\")\n",
        "HEADERS = ({'User-Agent': \n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
        "            'Accept-Language': 'en-US, en;q=0.5'})\n",
        "f = open('scrapped_amazon_general_queries_data.csv', 'w+', encoding=\"utf-8\")\n",
        "fieldnames = ['productTitle', 'productDescription', 'Rating', 'Price', 'Brand', 'Ingredients', 'About this item',\n",
        "              'Frequently bought together', 'Products related to this item', 'URL']\n",
        "writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
        "\n",
        "writer.writeheader()\n",
        "\n",
        "## data cleaning\n",
        "\n",
        "# list of attrs needed = ['productTitle', 'productDescription', 'Rating', 'Price', 'Brand', 'Ingredients', 'About this item',\n",
        "#                           'Frequently bought together', 'Products related to this item']\n",
        "url_data = url_data.dropna(subset=['URL'])\n",
        "for i in range(len(url_data)):\n",
        "    data = url_data.iloc[i]\n",
        "    URL = data.URL\n",
        "    webpage = requests.get(URL, headers=HEADERS)\n",
        "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
        "\n",
        "    # Extracting Title\n",
        "    try:\n",
        "        title = soup.find(\"span\",attrs={\"id\":'productTitle'})\n",
        "        # print(\"at 1 title is\", title)\n",
        "        title = title.string.strip().replace(',', '')\n",
        "        # print(\"at 1 title is\", title)\n",
        "    #         writer.writerow({'productTitle': title})\n",
        "    except:\n",
        "        title = np.nan\n",
        "\n",
        "    # print(\"title:\", title)\n",
        "\n",
        "    # Extracting Product Descriptions\n",
        "    try:\n",
        "        desc = soup.find(\"div\", attrs={\"id\":'productDescription'}).find(\"span\").string\n",
        "    #         writer.writerow({'productDescription': desc})\n",
        "    except:\n",
        "        desc = np.nan\n",
        "\n",
        "    # Rating and Price\n",
        "    rating = data.Rating\n",
        "\n",
        "#     try:\n",
        "#         rating = soup.find(\"span\",attrs={\"data-hook\":'acr-average-starts-rating-text'})\n",
        "#     except:\n",
        "#         rating = np.nan\n",
        "\n",
        "    price = data.Price\n",
        "\n",
        "#     try:\n",
        "#         price = soup.find(\"span\",{\"id\":'snsDetailPagerPrice', \"data-value\": True})['data-value'] #.find(\"span\",attrs={\"id\":'sns-base-price'}).string\n",
        "#         print(\"price:\",price)\n",
        "#     except:\n",
        "#         price = np.nan\n",
        "\n",
        "    # No. of reviews\n",
        "    try:\n",
        "        reviews = soup.find(\"span\",{\"id\":\"acrCustomerReviewText\"}).string\n",
        "    #         writer.writerow({'productDescription': reviews})\n",
        "    except:\n",
        "        reviews = np.nan\n",
        "\n",
        "    # Brand\n",
        "    try:\n",
        "        lis = soup.find(\"table\", attrs={\"class\":\"a-normal a-spacing-micro\"}).find_all(\"span\")\n",
        "    #         print(\"lis \",lis)\n",
        "        for idx, val in enumerate(lis):\n",
        "            if(val.string==\"Brand\"):\n",
        "                brand = lis[idx+1].string\n",
        "                break\n",
        "    #         writer.writerow({'Brand': brand})\n",
        "        if brand is None:\n",
        "            brand = np.nan\n",
        "    except:\n",
        "        brand = np.nan\n",
        "\n",
        "    # print(\"here, brand:\",brand)\n",
        "    # Ingredients\n",
        "    try:\n",
        "        flag=0\n",
        "        lis = soup.find_all(\"div\", attrs={\"class\" : \"a-section content\"})\n",
        "        for idx, val in enumerate(lis):\n",
        "            st_list = val.find_all(re.compile(\"h\"))\n",
        "            for idx2, val2 in enumerate(st_list):\n",
        "                if(val2.string==\"Ingredients\"):\n",
        "                    ingredients = lis[idx].text\n",
        "                    flag=1\n",
        "                    break\n",
        "            if(flag):\n",
        "                break\n",
        "        if ingredients is None:\n",
        "            ingredients = np.nan\n",
        "    #         writer.writerow({'Ingredients': ingredients})\n",
        "    except:\n",
        "        ingredients = np.nan\n",
        "\n",
        "\n",
        "    # About this item\n",
        "    try:\n",
        "        lis = soup.find(\"div\", attrs={\"id\":\"feature-bullets\"}).find_all(\"li\")\n",
        "        features = []\n",
        "        for val in lis:\n",
        "            features.append(val.string)\n",
        "    #         writer.writerow({'About this item': features})\n",
        "    except:\n",
        "        features = np.nan \n",
        "\n",
        "    # Frequently bought together\n",
        "    # okay\n",
        "\n",
        "    try:\n",
        "        fbt = soup.find(\"div\", attrs={\"class\": \"cardRoot bucket\"}).find_all(\"a\", href=True)\n",
        "        fbt_links = []\n",
        "        for val in fbt:\n",
        "            fbt_links.append('https://www.amazon.com/'+val['href'])\n",
        "    #         writer.writerow({'Frequently bought together': fbt_links})\n",
        "    except:\n",
        "        fbt_links = np.nan\n",
        "\n",
        "    # Product related to this item\n",
        "    try:\n",
        "        prt = soup.find(\"div\", {\"id\":\"sims-consolidated-2_feature_div\"}).find_all(\"a\", href=True)\n",
        "        dic = {}\n",
        "        for idx, val in enumerate(prt):\n",
        "            try:\n",
        "                temp = val['title']\n",
        "                dic[temp] = 1\n",
        "            except:\n",
        "                temp=1\n",
        "\n",
        "        prt_list_title = list(dic.keys())\n",
        "    except:\n",
        "        prt_list_title = np.nan\n",
        "        \n",
        "    writer.writerow({'productTitle': title, 'productDescription': desc, 'Rating': rating, 'Price': price, 'Brand': brand, 'Ingredients': ingredients, 'About this item': features, 'Frequently bought together': fbt_links,'URL': URL})\n",
        "    \n",
        "\n",
        "csvreader = csv.reader(x.replace('nan', '') for x in f)\n",
        "f.close()                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3972c47c",
      "metadata": {
        "id": "3972c47c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}